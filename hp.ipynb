{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from py3k_fix import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-k','ipython.move-selected-cell-up')\n",
    "IPython.keyboard_manager.command_shortcuts.add_shortcut('Ctrl-j','ipython.move-selected-cell-down')\n",
    "IPython.keyboard_manager.command_shortcuts.add_shortcut('Shift-m','ipython.merge-selected-cell-with-cell-after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter, defaultdict, OrderedDict\n",
    "from functools import wraps, partial\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import operator as op\n",
    "from operator import itemgetter as itg, attrgetter as prop, methodcaller as mc\n",
    "from os.path import join\n",
    "import re\n",
    "from scipy import stats\n",
    "import toolz.curried as z\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "p = print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pat = re.compile(r'.+? HP (\\d+).+')\n",
    "for fn in glob('src/orig/*.rtf'):\n",
    "    print(fn)\n",
    "    [i] = pat.findall(fn)\n",
    "    rtfdst = join('src', 'rtf', 'hp{}.rtf'.format(i))\n",
    "    txtdst = join('src', 'txt', 'hp{}.txt'.format(i))\n",
    "    !cp \"$fn\" \"$rtfdst\"\n",
    "    !unoconv --format=txt --output=$txtdst $rtfdst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    with open(\"src/txt/hp1.txt\",'rb') as f:\n",
    "        txt = f.read().decode(\"utf-8-sig\")\n",
    "    #     doc = Rtf15Reader.read(f)\n",
    "    t = txt[:60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('src/nltk_stopwords.txt', 'r') as f:\n",
    "with open('src/stops.txt', 'r') as f:\n",
    "    stops = set(l for l in f.read().splitlines() if l and not l.startswith('#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Chapter = namedtuple('Chapter', 'num title text')\n",
    "\n",
    "\n",
    "class Book(object):\n",
    "    def __init__(self, title, chapters: {int: Chapter}):\n",
    "        self.chapters = chapters\n",
    "        self.title = title\n",
    "        self.txts = OrderedDict()\n",
    "        for n, chap in sorted(chapters.items()):\n",
    "            setattr(self, 't{}'.format(n), chap.text)\n",
    "            self.txts[n] = chap.text\n",
    "        txt = reduce(op.add, self.txts.values())\n",
    "        self.txt = clean_text(txt)\n",
    "\n",
    "\n",
    "class BookSeries(object):\n",
    "    def __init__(self, n=7):\n",
    "        bks = {i: parsebook(i, vb=1) for i in range(1, n + 1)}\n",
    "        \n",
    "        self.txts = OrderedDict()\n",
    "        for n, bk in sorted(bks.items()):\n",
    "            setattr(self, 'b{}'.format(n), bk.txt)\n",
    "            self.txts[n] = bk.txt\n",
    "        txt = reduce(op.add, self.txts.values())\n",
    "        self.txt = clean_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bookpat = re.compile(r'''\\A(?P<title>.+)\n",
    "(?!(.+)+?)\n",
    "(?P<body>(Chapter 1)\\n+(.+\\n+)+)''')\n",
    "\n",
    "bookpat = re.compile(r'''\\A(?P<title>.+)\n",
    "\\n*\n",
    "(?:(?:.+\\n+)+?)\n",
    "(?P<body>\n",
    "    (Chapter\\ 1)\n",
    "    \\n+\n",
    "    (.+\\n*)+\n",
    ")''', re.VERBOSE)\n",
    "\n",
    "chappat = re.compile(r'''(Chapter (\\d+)\\n+((?:.+\\n+)+))+''')\n",
    "chapsep = re.compile(r'Chapter (\\d+)\\n(.+)\\n+')\n",
    "# m = bookpat.match(t)\n",
    "# gd = m.groupdict()\n",
    "# title = gd['title']\n",
    "# body = gd['body']\n",
    "# gd\n",
    "# m\n",
    "# m.groupdict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "book = {int(chnum): Chapter(int(chnum), title, text) for chnum, title, text in z.partition(3, chs)}\n",
    "bk = Book(book)\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsebook(fn=\"src/txt/hp1.txt\", vb=False):\n",
    "    global txt\n",
    "    p = print if vb else (lambda *x, **y: None)\n",
    "    if isinstance(fn, int):\n",
    "        fn = \"src/txt/hp{}.txt\".format(fn)\n",
    "    p('Reading {}'.format(fn)) \n",
    "    with open(fn,'rb') as f:\n",
    "        txt = f.read().decode(\"utf-8-sig\")\n",
    "        \n",
    "    gd = bookpat.search(txt).groupdict()\n",
    "    \n",
    "    booktitle = gd['title']\n",
    "    body = gd['body']\n",
    "\n",
    "    chs = chapsep.split(body)[1:]\n",
    "    book = {int(chnum): Chapter(int(chnum), title, text) for chnum, title, text in z.partition(3, chs)}\n",
    "    return Book(booktitle, book)\n",
    "\n",
    "\n",
    "def clean_text(t):\n",
    "    reps = {\n",
    "        '’': \"'\",\n",
    "        '‘': \"'\",\n",
    "        '“': '\"',\n",
    "        '”': '\"',\n",
    "        '\\xad': '',\n",
    "        '—': '-',\n",
    "       }\n",
    "    \n",
    "    def rep(s, frto):\n",
    "        fr, to = frto\n",
    "        return s.replace(fr, to)\n",
    "    t = reduce(rep, reps.items(), t)\n",
    "    return t\n",
    "\n",
    "# bk = parsebook()\n",
    "bks = BookSeries(5)\n",
    "bksall = BookSeries(7)\n",
    "# bk = bks.b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bk.txt[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(filter(lambda x: not re.match(r'[\\dA-Za-z \\.\"\\'\\-\\(\\);:!\\?,\\n]', x), bks.b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Complexity\n",
    "- avg word length (syllables?)\n",
    "- avg sentence length\n",
    "\n",
    "- Words: 129 | Syllables: 173 | Sentences: 7 | Characters: 568 | Adverbs: 4\n",
    "Characters/Word: 4.4 | Words/Sentence: 18.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Text\n",
    "I recently saw the [spaCy](https://spacy.io) library, which bills itself as a \"library for industrial-strength natural language processing in Python and Cython,\" and this seemd like a good opportunity to explore it. The starting point is a parsing function that parses, tags and detects entities all in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Series\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.notebook_repr_html = False\n",
    "pd.options.display.width = 120\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy.parts_of_speech as ps\n",
    "import spacy.en\n",
    "from spacy.en import English\n",
    "\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokens = nlp(bks.b1, tag=True, parse=True, entity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bktks = {i: nlp(bktxt, tag=True, parse=True, entity=True) for i, bktxt in bks.txts.items()}\n",
    "bktksall = {i: nlp(bktxt, tag=True, parse=True, entity=True) for i, bktxt in bksall.txts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tobooks(f, bks=bktks):\n",
    "    return pd.concat([f(v, i) for i, v in bks.items()])\n",
    "\n",
    "def booker(f: 'toks -> [str]') -> '(toks, int) -> DataFrame':\n",
    "    @wraps(f)\n",
    "    def tobookdf(toks, bknum):\n",
    "        df = DataFrame(f(toks), columns=['Val'])\n",
    "        df['Book'] = bknum\n",
    "        return df\n",
    "    return tobookdf\n",
    "    \n",
    "over_books = z.comp(partial(tobooks, bks=bktksall), booker)\n",
    "\n",
    "sent_lens = booker(lambda parsed: [spanlen(sent) for sent in parsed.sents])\n",
    "wd_lens = booker(lambda parsed: [len(tok) for tok in parsed if len(tok) > 1])\n",
    "spanlen = lambda span: len([wd for wd in span if len(wd) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wd_sent_lens():\n",
    "    wd_len_ = tobooks(wd_lens, bks=bktksall)\n",
    "    wd_len = wd_len_.groupby('Book')['Val'].agg(['mean', 'median', 'std'])\n",
    "    \n",
    "    sent_len_ = tobooks(sent_lens, bks=bktksall)\n",
    "    sent_len = sent_len_.groupby('Book')['Val'].agg(['mean', 'median', 'std'])\n",
    "    lens = {'Sentence_length': sent_len, 'Word_length': wd_len}\n",
    "    return pd.concat(lens.values(), axis=1, keys=lens.keys())\n",
    " \n",
    "wd_sent_lens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assertert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There doesn't seem to be a discernible difference in the average word length between the books. This could be because even complex language is largely composed of shorter, commoner words, highlighted by rarer, more complex words. A way to test this could be to somehow get a measure of the frequency of just the rarer words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tt = bktks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_words(parsed):\n",
    "    \"Non-capitalized words > 3 chars long that aren't stopwords\"\n",
    "    wds = [tok.orth_ for tok in parsed]\n",
    "    #wds = [tok.string.rstrip() for tok in parsed]\n",
    "    return [w for w in wds if len(w) > 3 and (w.lower() not in stops) and not w[0].isupper()]\n",
    "\n",
    "def wd_freqs(parsed):\n",
    "    vcs = Series(Counter(reg_words(parsed))).sort_values(ascending=False)\n",
    "    return vcs\n",
    "    return vcs[vcs < 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folllowing shows the relative word frequency distribution. The first two numbers in the first column indicate that for book one, words appearing only 1 time account for 45.2% of all the word occurences, while words appearing twice account for 16.9%. If anything, it appears that the share of rare words (those appearing only once or twice) decrease with each book, rather than increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uncwds = over_books(wd_freqs).reset_index(drop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "wdfreq = DataFrame({bknum: gdf.Val.value_counts(normalize=1)[:k]\n",
    "            for bknum, gdf in uncwds.groupby(['Book'])})\n",
    "wdfreq = (wdfreq * 100).round(1)\n",
    "wdfreq.columns.name, wdfreq.index.name = 'Book', 'Word_freq'\n",
    "wdfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative share of words appearing 10 times or less also doesn't seem to indicate an increasing share of uncommon words, and if anything points to uncommon words being used more in the first three books, and deacreasing for the last four. (The following graph should be interpreted to say that, for example, 90% of the words in the first book are those that appear fewer than 11 times, while 86% of the words in book 5 occur fewer than 11 times).\n",
    "\n",
    "**artifact of fewer pages**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdfreq.apply(mc('cumsum')).ix[10].plot(title='Share of words in each book that appear 10 times or less')\n",
    "plt.ylabel('% of words in each book that appear 10 times or less');\n",
    "assert False, 'Format?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, however, only counts words that are rare within the context of this book. spaCy provides a log-probability score for each parsed word, based on its frequency in external corpora. These will be negative numbers such that a lower score indicates that a word is less common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = lambda x: [tok.prob for tok in x if tok.is_lower]\n",
    "prob_books = over_books(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def percentile(q: float) -> \"[float] -> int\":\n",
    "    def f(s):\n",
    "        return np.percentile(s, q)\n",
    "    f.__name__ = 'perc_{}'.format(q)\n",
    "    return f\n",
    "\n",
    "def show_freq(bookstats):\n",
    "    probstats = (bookstats.groupby('Book').Val\n",
    "                 .agg(['mean', 'std', 'median',\n",
    "                       percentile(5), percentile(25)])\n",
    "                .rename(columns=str.capitalize))\n",
    "    probstats[['Perc_5', 'Perc_25', 'Median', 'Mean']].plot(title='Word Frequency')\n",
    "    plt.xticks(range(1, 8));\n",
    "    return probstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_freq(prob_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most drastic difference is in the frequency of the 95th percentile between first and second books. The graph shows that a typical word in the 95th percentile has a log probability of -13.3 in the first book and -13.8 in the second. It doesn't look that drastic, and there doesn't seem to be a discernable overall trend, either.\n",
    "\n",
    "Out of curiosity, it could be helpful to dig into what the probabilities look like for the first couple hundred words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_unc_word_trend():\n",
    "    n = 200\n",
    "    s1 = Series(probs1).sort_values(ascending=True).reset_index(drop=1)\n",
    "    s1[s1 < -12][:n].plot()\n",
    "    s2 = Series(probs2).sort_values(ascending=True).reset_index(drop=1)\n",
    "    s2[s2 < -12][:n].plot(title='Log probability for $n$ rarest words')\n",
    "    plt.legend(['Book 1', 'Book 2'])\n",
    "    \n",
    "show_unc_word_trend()\n",
    "xo, xi = plt.xlim()\n",
    "plt.hlines([-18.25], xo , xi, linestyles='dashdot')\n",
    "plt.hlines([-18.32], xo , xi, linestyles='dashdot');\n",
    "assert False, \"Resize plot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the least common words, it looks like the part of the reason Book 2's words are less frequent is due to a few streaks of words that have log probabilities indicated by the dashed lines. The repetition of certain uncommon words in the story line could lead us to classify some text as more complex than we should. A solution would be to run the same plots on the probabilities of *unique* words in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prob_id(toks) -> 'DataFrame[Prob, Id]':\n",
    "    return DataFrame([(tok.prob, tok.orth) for tok in toks if tok.is_lower], columns=['Prob', 'Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unique_probs(toks):\n",
    "    \"Like `probs`, but drop duplicate words\"\n",
    "    df = get_prob_id(toks)\n",
    "    return df.drop_duplicates('Id').Prob.tolist()\n",
    "\n",
    "uprob_books = over_books(unique_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufreq = show_freq(uprob_books)\n",
    "ufreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the trend towards more complex words is much more pronounced, and looks as if it continues throughout the whole series, with book 5 having disproportionately many more complex words. As anyone who's read the series can tell, Book 5 (*Order of the Phoenix*) also stands out as being disproportionately longer in page numbers, as confirmed by the wordcount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = Series(z.valmap(len, bktksall))\n",
    "wc.plot(title='Word count'); plt.ylim(0, None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which could lead us to wonder whether the increasing complexity in word choice is simply an artifact of the length of the books (if the text were generated randomly from the same distribution, we would expect longer texts to include a greater number of unique and rarer words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(wc, ufreq.Mean); plt.ylabel('Mean log p'); plt.xlabel('Total word count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_corrcoef(x=None, y=None, data=None):\n",
    "    sns.regplot(x=x, y=y, data=data)\n",
    "    plt.title('Corr. Coef.: {:.3f}'.format(stats.pearsonr(data[x], data[y])[0]))\n",
    "    plt.ylabel('Mean log p')\n",
    "    plt.xlabel('Total word count');\n",
    "    \n",
    "plot_corrcoef(x='Word_count', y='Mean', data=ufreq.assign(Word_count=wc))\n",
    "# sns.regplot(x='Word_count', y='Mean', data=ufreq.assign(Word_count=wc))\n",
    "# plt.title('Corr. Coef.: {:.3f}'.format(stats.pearsonr(ufreq.Mean, wc)[0]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simgrowth(toks, nsims=20):\n",
    "    def simgrowth_():\n",
    "        s = set()\n",
    "        l = []\n",
    "        tks = map(prop('orth'), toks)\n",
    "        nr.shuffle(tks)\n",
    "        for w in tks:\n",
    "            s.add(w)\n",
    "            l.append(len(s))\n",
    "        return l\n",
    "    return [simgrowth_() for _ in range(nsims)]\n",
    "\n",
    "ls = simgrowth(bktks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in ls:\n",
    "    plt.plot(l, alpha=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ls5 = simgrowth(bktks[5])\n",
    "plt.figure(figsize=(16, 10))\n",
    "for l in ls5:\n",
    "    plt.plot(l, alpha=.05)\n",
    "    \n",
    "for l in ls:\n",
    "    plt.plot(l, alpha=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the relationship between typical word appears to have a quite [log] linear relationship with word count. I'm not sure what relationship is to be expected, but it looks like it would be worthwhile to try and correct for document length in determining word complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def sim(df, seed=None, aggfunc=None, size=None):\n",
    "\n",
    "    dd = (df.sample(n=size, replace=False, random_state=seed\n",
    "                   ).drop_duplicates('Id').Prob)\n",
    "    # with replacement, the distribution gets biased\n",
    "    # towards more low-probability words\n",
    "    return aggfunc(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_gen_text(worddist=5, sizebook=1, nsims=10000,\n",
    "                 aggfunc=np.median, n_jobs=0, vb=False):\n",
    "    pt = print if vb else (lambda *x, **_: None)\n",
    "    sizedf = get_prob_id(bktksall[sizebook])\n",
    "    size = len(sizedf)\n",
    "    df = get_prob_id(bktksall[worddist])\n",
    "    \n",
    "#     def sim(_):\n",
    "#         dd = (df.sample(n=size, replace=False,random_state=next(seed)\n",
    "#                        ).drop_duplicates('Id').Prob)\n",
    "#         # with replacement, the distribution gets biased\n",
    "#         # towards more low-probability words\n",
    "#         return aggfunc(dd)\n",
    "    \n",
    "    mu = aggfunc(df.drop_duplicates('Id').Prob)\n",
    "    pt(mu)\n",
    "    if len(df) == size:\n",
    "        return [mu for _ in range(nsims)]\n",
    "#         sim = lambda _: mu\n",
    "        \n",
    "    if len(df) < size:\n",
    "        raise ValueError(\"Can't sample with replacement\"\n",
    "                         \" from smaller distribution\")\n",
    "    f = delayed(sim) if n_jobs else sim\n",
    "    gen = (f(df, seed=seed, aggfunc=aggfunc, size=size) for seed in range(nsims))\n",
    "    if n_jobs:\n",
    "        pt('Running {} jobs...'.format(n_jobs), end=' ')\n",
    "        ret = Parallel(n_jobs=n_jobs)(gen)\n",
    "    else:\n",
    "        ret = list(gen)\n",
    "    pt('Done.')\n",
    "    sys.stdout.flush()\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time xp = sim_gen_text(worddist=5, sizebook=1, nsims=1000, aggfunc=np.mean, n_jobs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time xp = sim_gen_text(worddist=5, sizebook=1, nsims=1000, aggfunc=np.mean, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time xp = sim_gen_text(worddist=5, sizebook=1, nsims=1000, aggfunc=np.mean, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time x = sim_gen_text(worddist=5, sizebook=1, nsims=100, aggfunc=np.mean, seed=count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time x = sim_gen_text(worddist=5, sizebook=5, nsims=100, aggfunc=np.mean, seed=count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gen_prob_text(nsims=10000, n_jobs=-1, usecache=True):\n",
    "    \"\"\"Run simulations and cache them (running ~10k sims\n",
    "    for each book) takes ~10 min.\"\"\"\n",
    "    fn = 'cache/gen_prob_text_{}.csv'.format(nsims)\n",
    "    \n",
    "    def gen_prob_text_read():\n",
    "        return pd.read_csv(fn).rename(columns=int)\n",
    "\n",
    "    if usecache:\n",
    "        try:\n",
    "            return gen_prob_text_read()\n",
    "        except IOError:\n",
    "            print('{} not found, running simulations...'.format(fn))\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    gens_mus = {\n",
    "        booknum: sim_gen_text(worddist=5, sizebook=booknum,\n",
    "                              nsims=nsims, aggfunc=np.mean, n_jobs=n_jobs)\n",
    "        for booknum in range(1, 8)\n",
    "    }\n",
    "    d = DataFrame(gens_mus)\n",
    "    d.to_csv(fn, index=None)\n",
    "    return gen_prob_text_read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time d = get_gen_prob_text(nsims=2000, n_jobs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm cache/gen_prob_text_2000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time d1 = get_gen_prob_text(nsims=20000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['Val', 'Book', 'Source']\n",
    "d = d1.copy()\n",
    "d.columns.name = 'Book'\n",
    "d = d.stack().sort_index(level='Book').reset_index(drop=0).rename(columns={0: 'Val'}).drop('level_0', axis=1)\n",
    "d['Source'] = 'Simulation'\n",
    "dboth = d[cols].append(uprob_books.assign(Source='Actual')[cols]).sort_values(['Book', 'Source'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bothagg = dboth.groupby(['Source', 'Book',]).mean()\n",
    "bothagg.unstack('Source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bothagg = dboth.groupby(['Source', 'Book',]).mean()\n",
    "bothagg.unstack('Source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.groupby('Book').Val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufreq.Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bk5sim = d.query('Book == 5')\n",
    "bk5mu = uprob_books.query('Book == 5').Val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "pbothagg = bothagg.ix['Actual'].copy().rename(columns={'Val': 'Actual'})\n",
    "pbothagg.index -= 1\n",
    "# pbothagg.plot(style='o', c='k', figsize=(16, 10))\n",
    "plt.scatter([], [], s=80, c='k', marker='x', linewidth=2)\n",
    "plt.legend(['Actual']);\n",
    "sns.violinplot('Book', 'Val', data=d)\n",
    "plt.scatter(pbothagg.index, pbothagg.Actual, s=80, c='k', marker='x', linewidth=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Percentiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DataFrame(Series({k: ((v.Val < ufreq.Mean[k]).mean() * 100).round(1) for k, v in d.groupby('Book')})).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='Word_count', y='Mean', data=simplot)\n",
    "simplot = d.groupby(['Book']).mean().rename(columns={'Val': 'Mean'}).assign(Word_count=wc)\n",
    "plt.title('Corr. Coef.: {:.3f}'.format(stats.pearsonr(simplot.Mean, wc)[0]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(gens_mu, bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Benchmark Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timer(f):\n",
    "    @wraps(f)\n",
    "    def f2(*a, **kw):\n",
    "        st = time.time()\n",
    "        ret = f(*a, **kw)\n",
    "        return time.time() - st, ret\n",
    "    return f2\n",
    "\n",
    "def run_timed(n_jobs=None, sims=[]):\n",
    "    return [timer(get_gen_prob_text)(nsims=nsims, n_jobs=n_jobs, usecache=False)[0] for nsims in sims]\n",
    "\n",
    "simtrials = [10, 20, 100, 300, 1000, 2000, 5000, 8000, 10000, 15000, 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nj0 = run_timed(n_jobs=0, sims=simtrials)\n",
    "nj2 = run_timed(n_jobs=2, sims=simtrials)\n",
    "njn1 = run_timed(n_jobs=-1, sims=simtrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DataFrame(OrderedDict([(0, nj0), (2, nj2), (-1, njn1)]), index=simtrials).plot(style='-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "secs = DataFrame(OrderedDict([(0, nj0), (2, nj2), (-1, njn1)]), index=simtrials) \n",
    "(secs // 60 + (secs % 60) * .01).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "secs.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(secs % 60) * .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!say done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DataFrame(OrderedDict([(0, nj0), (2, nj2), (-1, njn1)]), index=simtrials).plot(style='-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.drop_duplicates('Id').Prob.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = gen[0]\n",
    "t.pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DataFrame(ufreq.Median).assign(Word_count=lambda x: wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, gdf in uprob_books.groupby('Book'):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsamp = 1000\n",
    "nr.seed(nsamp)\n",
    "pd.np.random.seed(nsamp)\n",
    "gdf.Val.sample(nsamp, replace=nsamp, random_state=1).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_wd_prob(probs: Series, nsims=100, nsamp=1000):\n",
    "    meds = Series([np.median(nr.choice(probs, size=nsamp, replace=True)) for _ in range(nsims)])\n",
    "    return meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_wd_prob_slow(probs: Series, nsims=100, nsamp=1000):\n",
    "    meds = Series([probs.sample(nsims, replace=True, random_state=None).median() for _ in range(nsamp)])\n",
    "    return meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time meds = bootstrap_wd_prob(gdf.Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uprob_med_plot = uprob_books.assign(Book=lambda x: x.Book - 1).groupby('Book').Val.median()\n",
    "del uprob_med_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(meds), len(meds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_books.groupby('Book').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simfunc = booker(partial(bootstrap_wd_prob, nsims=1000, nsamp=1000))\n",
    "sims = pd.concat([simfunc(gdf.Val, bk) for bk, gdf in uprob_books.groupby('Book')]).reset_index(drop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simfunc = booker(partial(bootstrap_wd_prob, nsims=1000, nsamp=2000))\n",
    "sims2k = pd.concat([simfunc(gdf.Val, bk) for bk, gdf in uprob_books.groupby('Book')]).reset_index(drop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simfunc = booker(partial(bootstrap_wd_prob, nsims=1000, nsamp=10000))\n",
    "sims1k10k = pd.concat([simfunc(gdf.Val, bk) for bk, gdf in uprob_books.groupby('Book')]).reset_index(drop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simfunc = booker(partial(bootstrap_wd_prob, nsims=10000, nsamp=1000))\n",
    "sims10k1k = pd.concat([simfunc(gdf.Val, bk) for bk, gdf in uprob_books.groupby('Book')]).reset_index(drop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.title('1000 sims, 10000 samps')\n",
    "sns.violinplot(x='Book', y='Val', data=sims10k1k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.title('1000 sims, 10000 samps')\n",
    "sns.violinplot(x='Book', y='Val', data=sims2k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uprob_books.groupby('Book').Val.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.title('1000 sims, 1000 samps')\n",
    "sns.violinplot(x='Book', y='Val', data=sims);\n",
    "plt.xlim(-.5, 6.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.title('200 sims, 1000 samps')\n",
    "sns.violinplot(x='Book', y='Val', data=sims2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.title('100 sims, 1000 samps')\n",
    "sns.violinplot(x='Book', y='Val', data=sims);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meds.hist(alpha=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsims = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meds = Series([gdf.Val.sample(nsims, replace=nsamp, random_state=None).median() for _ in range(nsamp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdf[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uprob_books[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs1 = probs(bktks[1])\n",
    "probs2 = probs(bktks[2])\n",
    "probs12 = probs1 + probs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_diff(p1, p2, justdiff=False):\n",
    "    pct1 = np.percentile(p1, 5)\n",
    "    pct2 = np.percentile(p2, 5)\n",
    "    if justdiff:\n",
    "        return pct2 - pct1\n",
    "    return pct1, pct2, pct2 - pct1\n",
    "\n",
    "print('Actual 95%-tile rarest words: Book 1: {:.2f}, Book 2: {:.2f}, Diff: {:.4f}'.format(*get_diff(probs1, probs2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_sims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simulate_pctile_diff(probs12, probs1, nsim=200):\n",
    "    probs12 = np.array(probs12)\n",
    "    N = len(probs12)\n",
    "    l1 = len(probs1)\n",
    "    l2 = N - l1\n",
    "    p = l1 / N\n",
    "    nr.seed(0)\n",
    "    \n",
    "    def shuffle_groups3():\n",
    "        g1 = nr.binomial(1, p , N)\n",
    "        p1 = probs12[g1 == 1]\n",
    "        p2 = probs12[g1 == 0]\n",
    "        return get_diff(p1, p2)\n",
    "    \n",
    "    for _ in range(nsim):\n",
    "        all_sims.append(shuffle_groups3())\n",
    "#     return Series([shuffle_groups3() for _ in range(nsim)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time simulate_pctile_diff(probs12, probs1, nsim=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filt = lambda x: ((x > -18.5) & (x < -18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_prob = DataFrame([(tok.orth_, tok.prob) for tok in bktks[2] if filt(tok.prob)], columns=['Word', 'Prob'])\n",
    "_prob.Prob = _prob.Prob.round(3)\n",
    "_prob[filt(_prob.Prob)]\n",
    "_prob.query('Prob == [-18.317, -18.257]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2[filt(s2)].value_counts(normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sall_sims = Series(all_sims)\n",
    "(sall_sims < get_diff(probs1, probs2)[2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sall_sims.hist(bins=100, normed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time sims1 = simulate_pctile_diff(probs12, probs1, nsim=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time sims = simulate_pctile_diff(probs12, probs1, nsim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time sims2 = simulate_pctile_diff(probs12, probs1, nsim=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims2.hist(bins=50, normed=1)\n",
    "sims1.hist(bins=50, normed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr.ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_books[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs(bktks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = prob_books.query('Val < -15.2 & Val > -16 & Book == 2').Val\n",
    "v.value_counts(normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "pt = sns.boxplot\n",
    "pt = sns.violinplot\n",
    "pp = prob_books.query('Val < -14')\n",
    "pt(x='Book', y='Val', data=pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "for k, gdf in prob_books.groupby('Book'):\n",
    "    gdf.query('Val < -8').Val.hist(bins=200, alpha=.2, normed=True)\n",
    "    \n",
    "    if k > 6:\n",
    "        break\n",
    "        \n",
    "plt.legend(range(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newix = [range(length) for bk, length in prob_books.Book.value_counts(normalize=0).sort_index().items()]\n",
    "pb2 = prob_books.copy()\n",
    "pb2['Ix2'] = np.concatenate(newix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pb2.pivot(index='Ix2', columns='Book', values='Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_books[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_books.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_cumsum(s, **kw):\n",
    "    cmsm = (s.sort_values(ascending=True)\n",
    "            .reset_index().reset_index()\n",
    "            .set_index('Val', drop=False)['level_0'].cumsum())\n",
    "    cmsm /= cmsm.max()\n",
    "    cmsm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdf.Val.sort_values(ascending=True).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))  # 196000\n",
    "g = sns.FacetGrid(prob_books[:].query('Val < -14'), row=\"Book\", aspect=8, size=2)\n",
    "g.map(plot_cumsum, 'Val'); None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))  # 196000\n",
    "g = sns.FacetGrid(prob_books[:].query('Val < -8'), row=\"Book\", aspect=8, size=2)\n",
    "g.map(plt.hist, 'Val', normed=True, bins=200); None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_books.Val.kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for k, gdf in prob_books.groupby('Book'):\n",
    "    gdf.query('Val < -8').Val.hist(bins=200, alpha=.2, normed=True)\n",
    "    \n",
    "    if k > 6:\n",
    "        break\n",
    "        \n",
    "plt.legend(range(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pbs = probs(tokens)\n",
    "pbs = prob_books(bktks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map(z.comp(str.strip, str), bktks[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_words = listify(z.map(z.comp(str.strip, str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_tks = bktks[2]\n",
    "pbdf = (DataFrame(zip(probs(_tks), get_words(_tks)))\n",
    "        .sort_values(0, ascending=False)\n",
    "        .rename(columns={0: 'Prob', 1: 'Word'})\n",
    "       .assign(Prob=lambda x: x.Prob.round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pbdf.query('Prob == -15.516')  # .Word.value_counts(normalize=0)\n",
    "pbdf.query('Prob == -10.963').Word.value_counts(normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pbdf.query('Prob < -8 & Prob > -16').Prob.value_counts(normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pbdf.query('Prob < -15.2 & Prob > -16').Prob.value_counts(normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!open /tmp/x.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "nr.seed(10)\n",
    "a = nr.randint(0, 20, (2, 3))\n",
    "df = pd.DataFrame(a)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.index.name = 'A0'\n",
    "df.columns.name = 'A1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.rename?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "This count is only a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tok in tokens:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok.prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "gdf.Val.value_counts(normalize=1)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uncwds.groupby(['Book', 'Val']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uncwds.Book.value_counts(normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uncwds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_freqs(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vcs = Series(Counter(reg_words(tt))).sort_values(ascending=False)\n",
    "bm = vcs.index.map(lambda x: len(x) > 3 and (x.lower() not in stops) and not x[0].isupper())\n",
    "vcs = vcs[bm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = 19\n",
    "DataFrame(list(z.partition(R, vcs.index[:R*20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Longest words\n",
    "Series([tok.string.strip() for bk in bktks.values() for tok in bk if len(tok) > 20]).value_counts(normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_lens = pd.concat([sent_lens(v, i) for i, v in bktks.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Longest sentences\n",
    "Series().value_counts(normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in [sent for bk in bktks.values() for sent in bk.sents if spanlen(sent) > 200]:\n",
    "    print(s, end='\\n=======\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[tok for tok in bktks[5] if len(tok) > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(x='Book', y=0, data=wd_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "sns.boxplot(x='Book', y=0, data=sent_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "pt = sns.boxplot\n",
    "pt = sns.violinplot\n",
    "pt(x='Book', y=0, data=uncwds)\n",
    "# plt.ylim(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_lens.groupby('Book')[0].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z.valmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(z.valmap(sent_lens, bktks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bktks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_lens(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "span.end - span.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for wd in list(span):\n",
    "    wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for span in tokens.sents:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "span.text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(list(tokens.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "span.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[tokens[i] for i in range(span.start, span.end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "span.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok.ent_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people = Series(ent.string.rstrip() for ent in tokens.ents if ent.label_ == 'PERSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pn = people.value_counts(normalize=0)\n",
    "pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pn[40:-40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ent in tokens.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tok in tokens[:40]:\n",
    "    print(tok, ps.NAMES[tok.pos])\n",
    "    1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Find Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wds = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps = Series([tok.string.rstrip() for tok in tokens if tok.is_title and tok.pos == ps.NOUN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps.value_counts(normalize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok.is_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps.PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pos in dir(ps)[8:]:\n",
    "    print(pos)\n",
    "    if pos.isupper():\n",
    "        int(getattr(ps, pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[getattr(ps, pos) for pos in dir(ps) if pos.isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "{getattr(ps, pos): pos for pos in dir(ps) if pos.isupper()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!open /Users/williambeard/miniconda3/envs/hp/lib/python3.5/site-packages/spacy/en/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
